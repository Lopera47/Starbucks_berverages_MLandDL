{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script is to build a multi classification model using neural links\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns \n",
    "from keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Beverage_category</th>\n",
       "      <th>Beverage</th>\n",
       "      <th>Beverage_prep</th>\n",
       "      <th>Calories</th>\n",
       "      <th>Total Fat (g)</th>\n",
       "      <th>Trans Fat (g)</th>\n",
       "      <th>Saturated Fat (g)</th>\n",
       "      <th>Sodium (mg)</th>\n",
       "      <th>Total Carbohydrates (g)</th>\n",
       "      <th>Cholesterol (mg)</th>\n",
       "      <th>Dietary Fibre (g)</th>\n",
       "      <th>Sugars (g)</th>\n",
       "      <th>Protein (g)</th>\n",
       "      <th>Vitamin A (% DV)</th>\n",
       "      <th>Vitamin C (% DV)</th>\n",
       "      <th>Calcium (% DV)</th>\n",
       "      <th>Iron (% DV)</th>\n",
       "      <th>Caffeine (mg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Brewed Coffee</td>\n",
       "      <td>Short</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Brewed Coffee</td>\n",
       "      <td>Tall</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Brewed Coffee</td>\n",
       "      <td>Grande</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>330.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>Brewed Coffee</td>\n",
       "      <td>Venti</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>410.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Classic Espresso Drinks</td>\n",
       "      <td>CaffÃƒÂ¨ Latte</td>\n",
       "      <td>Short Nonfat Milk</td>\n",
       "      <td>70</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        Beverage_category        Beverage      Beverage_prep  \\\n",
       "0           0                   Coffee   Brewed Coffee              Short   \n",
       "1           1                   Coffee   Brewed Coffee               Tall   \n",
       "2           2                   Coffee   Brewed Coffee             Grande   \n",
       "3           3                   Coffee   Brewed Coffee              Venti   \n",
       "4           4  Classic Espresso Drinks  CaffÃƒÂ¨ Latte  Short Nonfat Milk   \n",
       "\n",
       "   Calories  Total Fat (g)  Trans Fat (g)   Saturated Fat (g)  Sodium (mg)  \\\n",
       "0         3            0.1             0.0                0.0            0   \n",
       "1         4            0.1             0.0                0.0            0   \n",
       "2         5            0.1             0.0                0.0            0   \n",
       "3         5            0.1             0.0                0.0            0   \n",
       "4        70            0.1             0.1                0.0            5   \n",
       "\n",
       "   Total Carbohydrates (g)  Cholesterol (mg)  Dietary Fibre (g)  Sugars (g)  \\\n",
       "0                        5                 0                  0           0   \n",
       "1                       10                 0                  0           0   \n",
       "2                       10                 0                  0           0   \n",
       "3                       10                 0                  0           0   \n",
       "4                       75                10                  0           9   \n",
       "\n",
       "   Protein (g)   Vitamin A (% DV)   Vitamin C (% DV)  Calcium (% DV)   \\\n",
       "0           0.3                0.0               0.0             0.00   \n",
       "1           0.5                0.0               0.0             0.00   \n",
       "2           1.0                0.0               0.0             0.00   \n",
       "3           1.0                0.0               0.0             0.02   \n",
       "4           6.0                0.1               0.0             0.20   \n",
       "\n",
       "   Iron (% DV)   Caffeine (mg)  \n",
       "0           0.0          175.0  \n",
       "1           0.0          260.0  \n",
       "2           0.0          330.0  \n",
       "3           0.0          410.0  \n",
       "4           0.0           75.0  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data\n",
    "data_starbucks = pd.read_csv(\"out\\processed_data_starbucks.csv\", encoding = 'cp1252')\n",
    "data_starbucks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in features and labels\n",
    "X = data_starbucks.iloc[:,4:].values # Values important as a way to get ready the inputs for the model\n",
    "y = data_starbucks[\"Beverage_category\"].values\n",
    "#y = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coffee' 'Classic Espresso Drinks' 'Signature Espresso Drinks'\n",
      " 'TazoÃ‚Â® Tea Drinks' 'Shaken Iced Beverages' 'Smoothies'\n",
      " 'FrappuccinoÃ‚Â® Blended Coffee' 'FrappuccinoÃ‚Â® Light Blended Coffee'\n",
      " 'FrappuccinoÃ‚Â® Blended CrÃƒÂ¨me']\n"
     ]
    }
   ],
   "source": [
    "print(data_starbucks[\"Beverage_category\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>241 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    1\n",
       "..  ..\n",
       "236  8\n",
       "237  8\n",
       "238  8\n",
       "239  8\n",
       "240  8\n",
       "\n",
       "[241 rows x 1 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.DataFrame(y)\n",
    "y.replace(['Coffee', 'Classic Espresso Drinks', \"Signature Espresso Drinks\", \"TazoÃ‚Â® Tea Drinks\", \"Shaken Iced Beverages\", \n",
    "           \"Smoothies\", \"FrappuccinoÃ‚Â® Blended Coffee\", \"FrappuccinoÃ‚Â® Light Blended Coffee\", \"FrappuccinoÃ‚Â® Blended CrÃƒÂ¨me\"], \n",
    "          [0, 1, 2, 3, 4, 5, 6, 7, 8], inplace=True)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_onehot = to_categorical(y)\n",
    "y_onehot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Split Train between train and validate / 49 is the 20% of the balanced dataset\n",
    "X_val = X_train[:49]\n",
    "partial_x_train = X_train[49:]\n",
    "\n",
    "y_val = y_train[:49]\n",
    "partial_y_train = y_train[49:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial neurallink + Dropout to avoid overfitting\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(15,))) # related with datatrain shape\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(9, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 5.2324 - acc: 0.4126 - val_loss: 1.0127 - val_acc: 0.7143\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 5.3632 - acc: 0.4825 - val_loss: 1.1028 - val_acc: 0.6735\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 5.3446 - acc: 0.4266 - val_loss: 1.0941 - val_acc: 0.6735\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 5.1292 - acc: 0.4406 - val_loss: 0.9301 - val_acc: 0.7551\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 5.2934 - acc: 0.4126 - val_loss: 0.9579 - val_acc: 0.7143\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 4.4144 - acc: 0.3776 - val_loss: 1.0121 - val_acc: 0.6939\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 4.1644 - acc: 0.4266 - val_loss: 0.7060 - val_acc: 0.7551\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 3.7637 - acc: 0.4825 - val_loss: 0.6980 - val_acc: 0.7551\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 4.2825 - acc: 0.3846 - val_loss: 0.8795 - val_acc: 0.7347\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 3.2779 - acc: 0.5105 - val_loss: 0.8014 - val_acc: 0.7143\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 3.4274 - acc: 0.4895 - val_loss: 0.7429 - val_acc: 0.7347\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 4.2759 - acc: 0.4056 - val_loss: 0.7755 - val_acc: 0.7143\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 3.9555 - acc: 0.4545 - val_loss: 0.9180 - val_acc: 0.6327\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 3.3358 - acc: 0.4336 - val_loss: 0.7564 - val_acc: 0.7347\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 2.6590 - acc: 0.5175 - val_loss: 0.7102 - val_acc: 0.7347\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 3.2157 - acc: 0.4685 - val_loss: 0.7710 - val_acc: 0.6531\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 2.5360 - acc: 0.5315 - val_loss: 0.8237 - val_acc: 0.6531\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 3.1488 - acc: 0.4406 - val_loss: 0.7055 - val_acc: 0.6735\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 2.9616 - acc: 0.4755 - val_loss: 0.7816 - val_acc: 0.6531\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 2.8983 - acc: 0.4336 - val_loss: 0.6799 - val_acc: 0.6939\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "proccesing = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs=20,\n",
    "                   batch_size=50,\n",
    "                   validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step - loss: 0.8193 - acc: 0.7551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8193354606628418, 0.7551020383834839]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate with test\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# La prediccion = 3 = TazoÃ‚Â® Tea Drinks\n",
    "print(np.argmax(predictions[0]))\n",
    "print(y_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0\n",
      "Fold  1\n",
      "Fold  2\n",
      "Fold  3\n",
      "Fold  4\n",
      "Fold  5\n",
      "Fold  6\n",
      "Fold  7\n",
      "Fold  8\n",
      "Fold  9\n"
     ]
    }
   ],
   "source": [
    "# Manual Cross validation due to ram problems\n",
    "k = 10\n",
    "num_val_samples = len(partial_x_train) // k\n",
    "num_epoch = 20\n",
    "all_history = []\n",
    "for i in range(k):\n",
    "    print(\"Fold \" , i)\n",
    "    val_data = partial_x_train[i*num_val_samples: (i+1) * num_val_samples] \n",
    "    val_targets = partial_y_train[i*num_val_samples: (i+1) * num_val_samples]\n",
    "    \n",
    "    partial_train_data = np.concatenate( \n",
    "    [partial_x_train[:i * num_val_samples],\n",
    "     partial_x_train[(i+1) * num_val_samples:]],\n",
    "     axis= 0 \n",
    "    )\n",
    "    \n",
    "    partial_train_targets = np.concatenate(\n",
    "    [partial_y_train[:i * num_val_samples],\n",
    "     partial_y_train[(i+1) * num_val_samples:]],\n",
    "     axis= 0   \n",
    "    )  \n",
    "    \n",
    "    history = model.fit(partial_train_data, partial_train_targets, epochs=num_epoch, batch_size =50,\n",
    "                        validation_data = (val_data, val_targets),\n",
    "                        verbose=0)\n",
    "    all_history.append(history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.928571\n",
       "1     0.914286\n",
       "2     0.935714\n",
       "3     0.921429\n",
       "4     0.935714\n",
       "5     0.935714\n",
       "6     0.914286\n",
       "7     0.914286\n",
       "8     0.921429\n",
       "9     0.907143\n",
       "10    0.907143\n",
       "11    0.921429\n",
       "12    0.907143\n",
       "13    0.900000\n",
       "14    0.900000\n",
       "15    0.907143\n",
       "16    0.914286\n",
       "17    0.907143\n",
       "18    0.900000\n",
       "19    0.892857\n",
       "dtype: float64"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean per line to cross validation\n",
    "all_acc_avg = pd.DataFrame(all_history).mean(axis=0) \n",
    "all_acc_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I made a \"manual\" cross validation, due to problems with ram and KerasClassifier. Nevertheless the code is below this box.\n",
    "\n",
    "### Conclusion ###\n",
    "#It was possible to make a multiclass classification through the use of a neural network with a great accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elvie\\AppData\\Local\\Temp\\ipykernel_19360\\778128600.py:3: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  classifier = KerasClassifier(build_fn=model, epochs=20, batch_size=50)\n"
     ]
    }
   ],
   "source": [
    "# Before cross validation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "classifier = KerasClassifier(build_fn=model, epochs=20, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU connection\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Save model\n",
    "from keras.models import load_model\n",
    "model.save('model.h5')\n",
    "classifier_final = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://5ba6f372-9195-4bb7-8f14-95272e9e7015/assets\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ram://a1dd5ffb-5239-40c4-8dce-c766762b0d44/variables/variables\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [178], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m cross_validate\n\u001b[0;32m      6\u001b[0m kfold_validacion\u001b[39m=\u001b[39mKFold(n_splits\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 7\u001b[0m results \u001b[39m=\u001b[39m cross_val_score(classifier, partial_x_train, partial_y_train, cv\u001b[39m=\u001b[39;49mkfold_validacion)\n",
      "File \u001b[1;32md:\\Barba\\Business\\Datascientist\\41Projects\\starbucks_project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[0;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[0;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[0;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    527\u001b[0m )\n\u001b[0;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\Barba\\Business\\Datascientist\\41Projects\\starbucks_project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m    269\u001b[0m         X,\n\u001b[0;32m    270\u001b[0m         y,\n\u001b[0;32m    271\u001b[0m         scorers,\n\u001b[0;32m    272\u001b[0m         train,\n\u001b[0;32m    273\u001b[0m         test,\n\u001b[0;32m    274\u001b[0m         verbose,\n\u001b[0;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    276\u001b[0m         fit_params,\n\u001b[0;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[0;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Barba\\Business\\Datascientist\\41Projects\\starbucks_project\\venv\\lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32md:\\Barba\\Business\\Datascientist\\41Projects\\starbucks_project\\venv\\lib\\site-packages\\joblib\\parallel.py:873\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    870\u001b[0m n_jobs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_effective_n_jobs\n\u001b[0;32m    871\u001b[0m big_batch_size \u001b[39m=\u001b[39m batch_size \u001b[39m*\u001b[39m n_jobs\n\u001b[1;32m--> 873\u001b[0m islice \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(itertools\u001b[39m.\u001b[39;49mislice(iterator, big_batch_size))\n\u001b[0;32m    874\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(islice) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    875\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\Barba\\Business\\Datascientist\\41Projects\\starbucks_project\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:268\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m    266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m--> 268\u001b[0m         clone(estimator),\n\u001b[0;32m    269\u001b[0m         X,\n\u001b[0;32m    270\u001b[0m         y,\n\u001b[0;32m    271\u001b[0m         scorers,\n\u001b[0;32m    272\u001b[0m         train,\n\u001b[0;32m    273\u001b[0m         test,\n\u001b[0;32m    274\u001b[0m         verbose,\n\u001b[0;32m    275\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    276\u001b[0m         fit_params,\n\u001b[0;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39mreturn_train_score,\n\u001b[0;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39mreturn_estimator,\n\u001b[0;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39merror_score,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m cv\u001b[39m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Barba\\Business\\Datascientist\\41Projects\\starbucks_project\\venv\\lib\\site-packages\\sklearn\\base.py:87\u001b[0m, in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     85\u001b[0m new_object_params \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39mget_params(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     86\u001b[0m \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m new_object_params\u001b[39m.\u001b[39mitems():\n\u001b[1;32m---> 87\u001b[0m     new_object_params[name] \u001b[39m=\u001b[39m clone(param, safe\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     88\u001b[0m new_object \u001b[39m=\u001b[39m klass(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnew_object_params)\n\u001b[0;32m     89\u001b[0m params_set \u001b[39m=\u001b[39m new_object\u001b[39m.\u001b[39mget_params(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Barba\\Business\\Datascientist\\41Projects\\starbucks_project\\venv\\lib\\site-packages\\sklearn\\base.py:68\u001b[0m, in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(estimator, \u001b[39m\"\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(estimator, \u001b[39mtype\u001b[39m):\n\u001b[0;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m safe:\n\u001b[1;32m---> 68\u001b[0m         \u001b[39mreturn\u001b[39;00m copy\u001b[39m.\u001b[39;49mdeepcopy(estimator)\n\u001b[0;32m     69\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(estimator, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    151\u001b[0m copier \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39m__deepcopy__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    152\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     y \u001b[39m=\u001b[39m copier(memo)\n\u001b[0;32m    154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     reductor \u001b[39m=\u001b[39m dispatch_table\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n",
      "File \u001b[1;32md:\\Barba\\Business\\Datascientist\\41Projects\\starbucks_project\\venv\\lib\\site-packages\\keras\\engine\\training.py:377\u001b[0m, in \u001b[0;36mModel.__deepcopy__\u001b[1;34m(self, memo)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__deepcopy__\u001b[39m(\u001b[39mself\u001b[39m, memo):\n\u001b[0;32m    376\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[1;32m--> 377\u001b[0m         new \u001b[39m=\u001b[39m pickle_utils\u001b[39m.\u001b[39;49mdeserialize_model_from_bytecode(\n\u001b[0;32m    378\u001b[0m             \u001b[39m*\u001b[39;49mpickle_utils\u001b[39m.\u001b[39;49mserialize_model_as_bytecode(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    379\u001b[0m         )\n\u001b[0;32m    380\u001b[0m         memo[\u001b[39mid\u001b[39m(\u001b[39mself\u001b[39m)] \u001b[39m=\u001b[39m new\n\u001b[0;32m    381\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    382\u001b[0m         \u001b[39m# See comment in __reduce__ for explanation\u001b[39;00m\n",
      "File \u001b[1;32md:\\Barba\\Business\\Datascientist\\41Projects\\starbucks_project\\venv\\lib\\site-packages\\keras\\saving\\pickle_utils.py:47\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[1;34m(serialized_model)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mGFile(dest_path, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     46\u001b[0m                 f\u001b[39m.\u001b[39mwrite(archive\u001b[39m.\u001b[39mextractfile(name)\u001b[39m.\u001b[39mread())\n\u001b[1;32m---> 47\u001b[0m model \u001b[39m=\u001b[39m save_module\u001b[39m.\u001b[39;49mload_model(temp_dir)\n\u001b[0;32m     48\u001b[0m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mrmtree(temp_dir)\n\u001b[0;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32md:\\Barba\\Business\\Datascientist\\41Projects\\starbucks_project\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\Barba\\Business\\Datascientist\\41Projects\\starbucks_project\\venv\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:933\u001b[0m, in \u001b[0;36mload_partial\u001b[1;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[0;32m    930\u001b[0m   loader \u001b[39m=\u001b[39m Loader(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0;32m    931\u001b[0m                   ckpt_options, options, filters)\n\u001b[0;32m    932\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mNotFoundError \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 933\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m    934\u001b[0m       \u001b[39mstr\u001b[39m(err) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m You may be trying to load on a different device \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    935\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mfrom the computational device. Consider setting the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    936\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mto the io_device such as \u001b[39m\u001b[39m'\u001b[39m\u001b[39m/job:localhost\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    938\u001b[0m root \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39mget(\u001b[39m0\u001b[39m)\n\u001b[0;32m    939\u001b[0m root\u001b[39m.\u001b[39mgraph_debug_info \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39madjust_debug_info_func_names(debug_info)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ram://a1dd5ffb-5239-40c4-8dce-c766762b0d44/variables/variables\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'."
     ]
    }
   ],
   "source": [
    "# Cross (There are problem with the RAM)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "kfold_validacion=KFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(classifier, partial_x_train, partial_y_train, cv=kfold_validacion)\n",
    "#print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n",
    "#results = cross_validate(classifier, partial_x_train, partial_y_train, cv=kfold_validacion, return_train_score=True, scoring=\"accuracy\")\n",
    "# print(results)\n",
    "# print(results.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06516d00843b8c39ee66fedfcd225928cbc04cfe85f3572d3578f41eee3b2249"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
